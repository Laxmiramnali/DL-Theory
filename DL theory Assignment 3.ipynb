{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110adc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax? 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer? 6. Name three ways you can produce a sparse model. 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout? 8. Practice training a deep neural network on the CIFAR10 image dataset: a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function. b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters. c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed? d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.). e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c984d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, it is generally acceptable to initialize the bias terms to 0. However, it is important to note that other values may result in better performance, depending on the type of model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Name three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELU is self-normalizing, meaning it does not suffer from the \"dying ReLU\" problem.\n",
    "\n",
    "SELU has a higher mean and variance than ReLU, which can help to reduce internal covariate shift and can lead to faster training.\n",
    "\n",
    "SELU is more robust to outliers and can maintain better performance under noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d55bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELU: When training deep neural networks and when dealing with noisy data or outliers.\n",
    "\n",
    "Leaky ReLU (and its variants): When training deep neural networks and when dealing with data that is not linearly separable.\n",
    "\n",
    "ReLU: When training deep neural networks and when dealing with data that is linearly separable.\n",
    "\n",
    "Tanh: When training shallow neural networks and when dealing with data that is non-linear.\n",
    "\n",
    "Logistic: When training shallow neural networks and when dealing with binary classification tasks.\n",
    "\n",
    "Softmax: When training shallow neural networks and when dealing with multi-class classification tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "If the momentum hyperparameter is set too close to 1, the SGD optimizer may cause oscillations in the optimization trajectory and can lead to slow convergence or even divergence of the optimization process. This is because the SGD optimizer will attempt to move too quickly in the direction of the previous update, resulting in overshooting the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70be596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use L1 regularization, which adds a penalty on the sum of the absolute values of the weights in the model. This encourages the model to reduce the number of non-zero weights, leading to a sparse model.\n",
    "\n",
    "Use feature selection techniques such as forward selection, backward selection, or recursive feature elimination to select only the most relevant features in the model. This can reduce the number of inputs, leading to a sparse model.\n",
    "\n",
    "Use pruning techniques such as magnitude pruning or low-rank factorization to remove redundant weights from the model. This can lead to a more efficient and sparse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b625c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout does slow down training, as it requires more iterations for the model to converge. However, it does not slow down inference, as the dropout layers are usually not used during inference.\n",
    "\n",
    "MC Dropout does slow down inference, as it requires multiple forward passes and additional computations to sample multiple weights from the dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "point of this exercise). Use He initialization and the ELU activation function.\n",
    "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.\n",
    "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it affect\n",
    "training speed?\n",
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "layers, etc.).\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
    "see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a98bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50066ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
